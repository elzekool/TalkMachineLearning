{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BBC Dataset demo\n",
    "This demo uses a freely available dataset from the BBC (http://mlg.ucd.ie/datasets/bbc.html). We will use this dataset as an example to automatcly cluster documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and extract dataset\n",
    "We will use the raw text dataset. We fill first download the ZIP if not done already and then extract it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract 2232 files from bbc-fulltext.zip\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "\n",
    "srcUrl = 'http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip'\n",
    "\n",
    "# Create folder if it doesn't exists\n",
    "if not os.path.exists('./bbc-corpus'):\n",
    "    print(\"Create bbc-corpus folder\")\n",
    "    os.makedirs('./bbc-corpus')\n",
    "\n",
    "# Check for existance of ZIP file\n",
    "if not os.path.exists('./bbc-corpus/bbc-fulltext.zip'):\n",
    "    print(\"Download %s\" % srcUrl)\n",
    "    urlretrieve(srcUrl, './bbc-corpus/bbc-fulltext.zip')\n",
    "    \n",
    "# Extract zipFile\n",
    "with ZipFile('./bbc-corpus/bbc-fulltext.zip', 'r') as zipFile:\n",
    "    print(\"Extract %d files from bbc-fulltext.zip\" % len(zipFile.namelist()))\n",
    "    zipFile.extractall('./bbc-corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract titles and text body\n",
    "We will go trough all the files and extract the titles and the body of each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rochus shocks Coria in Auckland\n",
      "Top seed Guillermo Coria went out of the Heineken Open in Auckland on Thursday with a surprise loss ...\n"
     ]
    }
   ],
   "source": [
    "# Create two lists\n",
    "item_titles = []\n",
    "item_texts = []\n",
    "\n",
    "# Go trough folders and trough files\n",
    "for subfolder in [ x for x in os.listdir('./bbc-corpus/bbc') if os.path.isdir('./bbc-corpus/bbc/' + x) ]:\n",
    "    for filename in os.listdir('./bbc-corpus/bbc/' + subfolder):\n",
    "        full_filename = './bbc-corpus/bbc/' + subfolder + '/' + filename            \n",
    "        file_contents = open(full_filename, 'r', encoding='iso 8859-15').read().split('\\n')\n",
    "        \n",
    "        # First line is title, then a empty line and then the main body\n",
    "        item_titles.append(file_contents[0])\n",
    "        item_texts.append('\\n'.join(file_contents[2:]))\n",
    "\n",
    "        \n",
    "# Test output\n",
    "print(item_titles[0])\n",
    "print(\"%s...\" % item_texts[0][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup item texts\n",
    "Cleanup the texts by first removing all non alpha characters. Change it to lowercase. And finally remove all stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top seed guillermo coria went heineken open auckland thursday surprise loss olivier rochus belgium coria lost semi final rochus goes face czech jan hernych winner jose acasuso argentina fifth seed fernando gonzalez eased past american robby ginepri chilean meet sixth seed juan ignacio chela next argentine beat potito starace rochus made semi finals australian hardcourt championships adelaide last week naturally delighted form two unbelievable weeks said today knew nothing lose beat great lost would losing top player coria conceded rochus played good added give best sad\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "for idx, item_text in enumerate(item_texts):\n",
    "    # Remove all non alpha characters and change it to lowercase\n",
    "    letters_only = re.sub('[^a-zA-Z]', ' ', item_texts[idx])\n",
    "    \n",
    "    # Split into words by space\n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stopwords_eng = set(stopwords.words(\"english\"))    \n",
    "    useful_words = [x for x in words if not x in stopwords_eng]\n",
    "    \n",
    "    # Store result\n",
    "    item_texts[idx] = ' '.join(useful_words)\n",
    "    \n",
    "# Test output\n",
    "print(item_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Tf-Idf vectors for all document terms\n",
    "Tf-Idf stands for *term frequencyâ€“inverse document frequency*. It is a numerical statistic that is intended to reflect how important a word is to a document in a collection. Words that are mentioned a lot in a document but are not mentioned in the whole collection are deemed important. Also words that are mentioned but are very common in the whole collection are not so important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 17047)\t0.0931072466343\n",
      "  (0, 14870)\t0.250494157871\n",
      "  (0, 7174)\t0.11442958744\n",
      "  (0, 3628)\t0.358187717552\n",
      "  (0, 18387)\t0.0563737957786\n",
      "  (0, 7534)\t0.0969661000928\n",
      "  (0, 11960)\t0.0465113632758\n",
      "  (0, 1017)\t0.131361856926\n",
      "  (0, 16908)\t0.0640594744112\n",
      "  (0, 16368)\t0.0669752067127\n",
      "  (0, 9937)\t0.0690257928222\n",
      "  (0, 11905)\t0.10043032211\n",
      "  (0, 14252)\t0.490173450729\n",
      "  (0, 1508)\t0.108932051169\n",
      "  (0, 9939)\t0.110435334484\n",
      "  (0, 14905)\t0.164902953089\n",
      "  (0, 5995)\t0.103380048144\n",
      "  (0, 6898)\t0.0746795583793\n",
      "  (0, 5712)\t0.04833491531\n",
      "  (0, 3986)\t0.0969661000928\n",
      "  (0, 8697)\t0.108932051169\n",
      "  (0, 7601)\t0.131361856926\n",
      "  (0, 18563)\t0.0647739164312\n",
      "  (0, 8881)\t0.0834980526236\n",
      "  (0, 83)\t0.138361489591\n",
      "  :\t:\n",
      "  (2224, 773)\t0.036520717039\n",
      "  (2224, 4201)\t0.0626900548222\n",
      "  (2224, 15063)\t0.0749579757996\n",
      "  (2224, 947)\t0.0389504153461\n",
      "  (2224, 8512)\t0.0349596315479\n",
      "  (2224, 8713)\t0.118509033808\n",
      "  (2224, 9466)\t0.133149384987\n",
      "  (2224, 8247)\t0.038686940249\n",
      "  (2224, 17012)\t0.102745662014\n",
      "  (2224, 16538)\t0.124781671098\n",
      "  (2224, 17958)\t0.0434138977688\n",
      "  (2224, 1860)\t0.0498130125671\n",
      "  (2224, 10709)\t0.0877536712942\n",
      "  (2224, 18834)\t0.088236084324\n",
      "  (2224, 13217)\t0.0315440780308\n",
      "  (2224, 64)\t0.0532834452347\n",
      "  (2224, 10710)\t0.0897706504303\n",
      "  (2224, 445)\t0.0438768356471\n",
      "  (2224, 17237)\t0.134655975646\n",
      "  (2224, 10974)\t0.23159390451\n",
      "  (2224, 4014)\t0.0578984761275\n",
      "  (2224, 16278)\t0.438830383756\n",
      "  (2224, 10975)\t0.0626900548222\n",
      "  (2224, 17413)\t0.626900548222\n",
      "  (2224, 11211)\t0.205874894074\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_words(words_list, stemmer):\n",
    "    return [stemmer.stem(word) for word in words_list]\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_words(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "# Create Tf-Idf vectors from our item_texts\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize)\n",
    "tfs = tfidf.fit_transform(item_texts)\n",
    "\n",
    "# Test output\n",
    "print(tfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See some nearest neighbors\n",
    "We will use nearest neighbors to see if by using cosine similarity we actually get usefull correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D'Arcy injury adds to Ireland woe\n",
      " - D'Arcy injury adds to Ireland woe (#11)\n",
      " - O'Driscoll out of Scotland game (#314)\n",
      " - Ireland surge past Scots (#468)\n",
      " - Ireland v USA (Sat) (#151)\n",
      " - Preview: Ireland v England (Sun) (#51)\n",
      " - O'Connor aims to grab opportunity (#366)\n",
      " - Italy 17-28 Ireland (#14)\n",
      " - O'Driscoll saves Irish blushes (#508)\n",
      " - Ireland call up uncapped Campbell (#111)\n",
      " - Ireland 17-12 South Africa (#301)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "neigh = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=10)\n",
    "neigh.fit(tfs)\n",
    "\n",
    "bill_id = 11\n",
    "\n",
    "print(item_titles[bill_id])\n",
    "for x in neigh.kneighbors(tfs[bill_id], return_distance=False).reshape([ -1 ]):\n",
    "    print(\" - %s (#%d)\" % (item_titles[x], x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
